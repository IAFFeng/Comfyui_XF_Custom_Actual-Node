# -*- coding: utf-8 -*-
"""Gemini Node Logic

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ec5YilsrWvL7YL9kfUuE-JDVSeaV2BK0
"""

import requests
import json
import torch
import numpy as np
from PIL import Image
import io
import base64
import time

class GeminiMattingNode:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "api_key": ("STRING", {"multiline": False, "default": "", "placeholder": "Enter your Gemini API Key (AIza...)"}),
                "mode": (["Transparent Background", "White Background"],),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "process_image"
    CATEGORY = "Gemini/Image"

    def tensor_to_base64(self, image):
        # Convert ComfyUI Tensor (batch, height, width, channels) to PIL Image
        i = 255. * image.cpu().numpy().squeeze()
        img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))

        # Save to buffer
        buffered = io.BytesIO()
        img.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode("utf-8"), img.size

    def base64_to_tensor(self, base64_string):
        img_data = base64.b64decode(base64_string)
        img = Image.open(io.BytesIO(img_data)).convert("RGBA")

        # Convert PIL back to Tensor
        img_np = np.array(img).astype(np.float32) / 255.0
        return torch.from_numpy(img_np).unsqueeze(0)

    def process_transparency_numpy(self, pil_image):
        """
        Ported from the React/Canvas logic: processTransparencyAdvanced
        Uses NumPy for faster processing in Python.
        """
        # Ensure RGBA
        img = pil_image.convert("RGBA")
        data = np.array(img)

        # Target Color: Bright Green (0, 255, 0)
        target_r, target_g, target_b = 0, 255, 0

        # Extract channels
        r, g, b, a = data[:,:,0], data[:,:,1], data[:,:,2], data[:,:,3]

        # Calculate Euclidean distance to pure green
        # distance = sqrt((r-0)^2 + (g-255)^2 + (b-0)^2)
        distance = np.sqrt(
            (r.astype(float) - target_r)**2 +
            (g.astype(float) - target_g)**2 +
            (b.astype(float) - target_b)**2
        )

        # Green Dominance logic: g > r*1.1 && g > b*1.1
        green_dominance = 1.1
        is_green_dominant = (g > (r * green_dominance)) & (g > (b * green_dominance))

        # Thresholds
        color_threshold = 90

        # Create masks
        # 1. Hard removal mask
        hard_mask = (distance < color_threshold) | (is_green_dominant & (g > 100))

        # 2. Soft edge mask (semi-transparent)
        # range: color_threshold to color_threshold + 30
        soft_mask_region = (distance >= color_threshold) & (distance < (color_threshold + 30))

        # Apply masks to Alpha channel
        # Set alpha to 0 for hard mask
        a[hard_mask] = 0

        # Calculate alpha for soft region
        if np.any(soft_mask_region):
            alpha_values = (distance[soft_mask_region] - color_threshold) / 30.0
            # Clip and scale to 0-255
            a[soft_mask_region] = np.clip(alpha_values * 255, 0, 255).astype(np.uint8)

        # Combine back
        data[:,:,3] = a
        return Image.fromarray(data)

    def process_image(self, image, api_key, mode):
        if not api_key:
            raise ValueError("API Key is required. Please enter your Google Gemini API Key.")

        base64_data, original_size = self.tensor_to_base64(image)

        # Determine Prompt based on Mode
        if mode == "Transparent Background":
            prompt = "Do not change the product. Remove the background and replace it with a pure, solid, bright GREEN color (Hex Code #00FF00). The background must be perfectly flat. High quality."
        else:
            prompt = "Do not change the product. Replace the background with a clean, solid pure white background (Hex Code #FFFFFF). High quality commercial photography."

        # Prepare API Request
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image-preview:generateContent?key={api_key}"

        payload = {
            "contents": [{
                "parts": [
                    {"text": prompt},
                    {
                        "inlineData": {
                            "mimeType": "image/png",
                            "data": base64_data
                        }
                    }
                ]
            }],
            "generationConfig": {
                "responseModalities": ["TEXT", "IMAGE"],
                "temperature": 0.4
            }
        }

        # API Call with Retry Logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    url,
                    headers={'Content-Type': 'application/json'},
                    data=json.dumps(payload)
                )

                if response.status_code != 200:
                    error_msg = response.text
                    if attempt == max_retries - 1:
                        raise Exception(f"API Error: {response.status_code} - {error_msg}")
                    time.sleep(1 * (attempt + 1))
                    continue

                result = response.json()

                # Parse Result
                candidates = result.get('candidates', [])
                if not candidates:
                    raise Exception("No candidates returned from API")

                parts = candidates[0].get('content', {}).get('parts', [])
                image_part = next((p for p in parts if 'inlineData' in p), None)

                if not image_part:
                    text_part = next((p for p in parts if 'text' in p), None)
                    if text_part:
                        print(f"Model returned text instead of image: {text_part['text']}")
                    raise Exception("Model did not return an image.")

                # Get Base64 result from API
                result_base64 = image_part['inlineData']['data']

                # Convert to PIL for processing
                img_data = base64.b64decode(result_base64)
                pil_img = Image.open(io.BytesIO(img_data)).convert("RGBA")

                # Resize back to original dimensions (as per original code logic)
                pil_img = pil_img.resize(original_size, Image.LANCZOS)

                # Post Processing
                if mode == "Transparent Background":
                    # Run the Green Screen Removal Algorithm
                    final_pil = self.process_transparency_numpy(pil_img)
                else:
                    # Just return the White Background image
                    final_pil = pil_img.convert("RGB") # Remove Alpha for white bg mode usually

                # Convert PIL back to Tensor for ComfyUI
                img_np = np.array(final_pil).astype(np.float32) / 255.0
                return (torch.from_numpy(img_np).unsqueeze(0),)

            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                time.sleep(1)

        return (image,)